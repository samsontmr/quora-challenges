{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input00 = open(\"input00.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(input00)\n",
    "numTrainingData, numFeatures = (int(s) for s in input00[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create numpy arrays to store training features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trainingFeatures = np.zeros(shape=(numTrainingData, numFeatures))\n",
    "trainingLabels = np.zeros(shape=(numTrainingData,), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFeatures = input00[1:numTrainingData+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean training dataset, remove unnecessary characters and store in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for entryNum in range(trainingFeatures.shape[0]):\n",
    "    entry = inputFeatures[entryNum].split()\n",
    "    #print(entry)\n",
    "    trainingLabels[entryNum] = float(entry[1])\n",
    "    params = [entry[i] for i in range(2, numFeatures + 2)]\n",
    "    cleanedParams = [param[param.index(\":\") + 1:] for param in params]\n",
    "    trainingFeatures[entryNum] = np.array([float(param) for param in cleanedParams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 23)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "#trainingFeatures = trainingFeatures[:,[6,7,13]]\n",
    "trainingFeatures = X_scaler.fit_transform(trainingFeatures)\n",
    "trainingFeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exract test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "numTestData = int(input00[numTrainingData+1])\n",
    "print(numTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testFeatures = np.zeros(shape=(numTestData, numFeatures))\n",
    "testNames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputTestFeatures = input00[numTrainingData+2:numTrainingData+2+numTestData]\n",
    "#print(inputTestFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputTestFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 ..., -1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "#trainingLabels = convert_to_onehot(trainingLabels,2)\n",
    "trainingLabels = np.array([i + 1 if i < 0 else i for i in trainingLabels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean test dataset, remove unnecessary characters and store in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entryNum in range(testFeatures.shape[0]):\n",
    "    entry = inputTestFeatures[entryNum].split()\n",
    "    testNames.append(entry[0])\n",
    "    params = [entry[i] for i in range(1, numFeatures + 1)]\n",
    "    cleanedParams = [param[param.index(\":\") + 1:] for param in params]\n",
    "    testFeatures[entryNum] = np.array([float(param) for param in cleanedParams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "#testFeatures = testFeatures[:,[6,7,13]] \n",
    "testFeatures = X_scaler.fit_transform(testFeatures)\n",
    "testFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_onehot(labels, num_classes):\n",
    "    labels = labels.reshape(-1)\n",
    "    return np.eye(num_classes, dtype=int)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(features, labels, idx, batch_size):\n",
    "    idx = idx % (features.shape[0] // batch_size)\n",
    "    if (idx+1) * batch_size < labels.shape[0]:\n",
    "        return features[idx * batch_size:(idx+1) * batch_size], labels[idx * batch_size:(idx+1) * batch_size]\n",
    "    else:\n",
    "        return features[idx * batch_size:], labels[idx * batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read labels for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = open(\"output00.txt\").readlines()\n",
    "content = [line.strip('\\n') for line in content]\n",
    "cleanedContent = [entry[entry.index(\" \") + 1:] for entry in content]\n",
    "\n",
    "testLabels = np.array([float(s) for s in cleanedContent], dtype=int)\n",
    "testLabels = np.array([i + 1 if i < 0 else i for i in testLabels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.44352513,  1.19620926,  0.16102307, ..., -1.1921951 ,\n",
       "       -0.18189074,  0.27827511])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingFeatures[:,1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainingFeatures[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLabels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "hidden_nodes_layer1 = 128\n",
    "\n",
    "hidden_nodes_layer2 = 64\n",
    "\n",
    "hidden_nodes_layer3 = 32\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float64)\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, trainingFeatures.shape[1]])\n",
    "y_ = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "W_hidden1 = tf.Variable(tf.random_normal([trainingFeatures.shape[1], hidden_nodes_layer1], dtype=tf.float64))\n",
    "b_hidden1 = tf.Variable(tf.random_normal([hidden_nodes_layer1], dtype=tf.float64))\n",
    "\n",
    "hidden1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden1), b_hidden1))\n",
    "\n",
    "\n",
    "W_hidden2 = tf.Variable(tf.random_normal([hidden_nodes_layer1, hidden_nodes_layer2], dtype=tf.float64))\n",
    "b_hidden2 = tf.Variable(tf.random_normal([hidden_nodes_layer2], dtype=tf.float64))\n",
    "\n",
    "hidden2 = tf.nn.relu(tf.add(tf.matmul(hidden1, W_hidden2), b_hidden2))\n",
    "\n",
    "W_hidden3 = tf.Variable(tf.random_normal([hidden_nodes_layer2, hidden_nodes_layer3], dtype=tf.float64))\n",
    "b_hidden3 = tf.Variable(tf.random_normal([hidden_nodes_layer3], dtype=tf.float64))\n",
    "\n",
    "hidden3 = tf.nn.relu(tf.add(tf.matmul(hidden2, W_hidden3), b_hidden3))\n",
    "\n",
    "dropout = tf.nn.dropout(hidden3, keep_prob=dropout_keep_prob)\n",
    "\n",
    "W_out = tf.Variable(tf.random_normal([hidden_nodes_layer3, num_labels], dtype=tf.float64))\n",
    "b_out = tf.Variable(tf.random_normal([num_labels], dtype=tf.float64))\n",
    "\n",
    "y = tf.add(tf.matmul(dropout, W_out), b_out)\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "merged = tf.summary.merge_all()\n",
    "train_step = tf.train.AdamOptimizer(learning_rate, epsilon=1e-8, beta1=0.9, beta2=0.999).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:719.6211680596717\n",
      "\n",
      "test loss: 518.902573509\n",
      "step 100:94.39944299153422\n",
      "step 200:15.298795104254777\n",
      "\n",
      "test loss: 14.0296561553\n",
      "step 300:3.4066979790526246\n",
      "step 400:1.4611944120457896\n",
      "\n",
      "test loss: 3.51291204139\n",
      "step 500:0.8004473880195582\n",
      "step 600:0.8626167478859974\n",
      "\n",
      "test loss: 2.73947248735\n",
      "step 700:0.6527927013386755\n",
      "step 800:0.8201253957887744\n",
      "\n",
      "test loss: 2.49268184612\n",
      "step 900:0.7785411256723811\n",
      "step 1000:0.6286360566325049\n",
      "\n",
      "test loss: 2.40280654009\n",
      "step 1100:0.6292043415959748\n",
      "step 1200:0.6147869485996895\n",
      "\n",
      "test loss: 2.3361854444\n",
      "step 1300:0.6186000125572491\n",
      "step 1400:0.6175159877512071\n",
      "\n",
      "test loss: 2.20746349211\n",
      "step 1500:0.6188202059823814\n",
      "step 1600:0.614920826850668\n",
      "\n",
      "test loss: 2.0852182151\n",
      "step 1700:0.6162225367327472\n",
      "step 1800:0.6111147142426\n",
      "\n",
      "test loss: 1.94994138755\n",
      "step 1900:0.611985153922199\n",
      "step 2000:0.5999186858791112\n",
      "\n",
      "test loss: 1.79677901167\n",
      "step 2100:0.5943383950560301\n",
      "step 2200:0.5947048257662809\n",
      "\n",
      "test loss: 1.64716836807\n",
      "step 2300:0.5894139239065762\n",
      "step 2400:0.5941652977719277\n",
      "\n",
      "test loss: 1.50639931304\n",
      "step 2500:0.5841207923552807\n",
      "step 2600:0.5776862452303503\n",
      "\n",
      "test loss: 1.32203774666\n",
      "step 2700:0.5780002176955836\n",
      "step 2800:0.5770959670145616\n",
      "\n",
      "test loss: 1.15495092519\n",
      "step 2900:0.5790130263363203\n",
      "step 3000:0.579434336466648\n",
      "\n",
      "test loss: 0.990627661515\n",
      "step 3100:0.5738745830759642\n",
      "step 3200:0.564940072663279\n",
      "\n",
      "test loss: 0.794963694613\n",
      "step 3300:0.5622149405935338\n",
      "step 3400:0.5576769217060379\n",
      "\n",
      "test loss: 0.648774452699\n",
      "step 3500:0.5700026827705555\n",
      "step 3600:0.5571499335511798\n",
      "\n",
      "test loss: 0.582947661528\n",
      "step 3700:0.5646661597196069\n",
      "step 3800:0.5501116923909883\n",
      "\n",
      "test loss: 0.503222139957\n",
      "step 3900:0.5442637647211653\n",
      "step 4000:0.5408804689527494\n",
      "\n",
      "test loss: 0.454145765568\n",
      "step 4100:0.5462507908568704\n",
      "step 4200:0.54695869149212\n",
      "\n",
      "test loss: 0.467519225919\n",
      "step 4300:0.5407613233878653\n",
      "step 4400:0.5267117565225354\n",
      "\n",
      "test loss: 0.454507596691\n",
      "step 4500:0.5391953885077031\n",
      "step 4600:0.5366957765981735\n",
      "\n",
      "test loss: 0.495027642677\n",
      "step 4700:0.5269070124324735\n",
      "step 4800:0.5102603114872373\n",
      "\n",
      "test loss: 0.443967278483\n",
      "step 4900:0.5280337186072027\n",
      "step 5000:0.5214775465718571\n",
      "\n",
      "test loss: 0.445019827864\n",
      "step 5100:0.5166422384467582\n",
      "step 5200:0.5196112489977361\n",
      "\n",
      "test loss: 0.43590513802\n",
      "step 5300:0.5075105581712877\n",
      "step 5400:0.5006860354276058\n",
      "\n",
      "test loss: 0.46062086402\n",
      "step 5500:0.5083511204779408\n",
      "step 5600:0.5046751967222992\n",
      "\n",
      "test loss: 0.429186063761\n",
      "step 5700:0.4905189471784648\n",
      "step 5800:0.4940384597951087\n",
      "\n",
      "test loss: 0.430645450561\n",
      "step 5900:0.49099921426550774\n",
      "step 6000:0.4828277773252784\n",
      "\n",
      "test loss: 0.42857050314\n",
      "step 6100:0.4951275010698555\n",
      "step 6200:0.47676250136443804\n",
      "\n",
      "test loss: 0.420499873336\n",
      "step 6300:0.4787590712582894\n",
      "step 6400:0.4796783518502394\n",
      "\n",
      "test loss: 0.477218061293\n",
      "step 6500:0.47596847941528037\n",
      "step 6600:0.4782755297359359\n",
      "\n",
      "test loss: 0.414224100012\n",
      "step 6700:0.47072368328174546\n",
      "step 6800:0.4733313513535332\n",
      "\n",
      "test loss: 0.434222942808\n",
      "step 6900:0.4616411141440926\n",
      "step 7000:0.4584069337169379\n",
      "\n",
      "test loss: 0.408942567655\n",
      "step 7100:0.47682802012913117\n",
      "step 7200:0.4680381281755046\n",
      "\n",
      "test loss: 0.461549355391\n",
      "step 7300:0.45933167938044567\n",
      "step 7400:0.4559752724177081\n",
      "\n",
      "test loss: 0.410010227729\n",
      "step 7500:0.45728843213296044\n",
      "step 7600:0.4534796464636388\n",
      "\n",
      "test loss: 0.413395445517\n",
      "step 7700:0.44503214265929625\n",
      "step 7800:0.4567572047558125\n",
      "\n",
      "test loss: 0.405955715895\n",
      "step 7900:0.45505348278549146\n",
      "step 8000:0.4418076733930671\n",
      "\n",
      "test loss: 0.407179318878\n",
      "step 8100:0.44265911393514495\n",
      "step 8200:0.43431677733171425\n",
      "\n",
      "test loss: 0.463283505779\n",
      "step 8300:0.43813957543508586\n",
      "step 8400:0.4333359726504276\n",
      "\n",
      "test loss: 0.482461689799\n",
      "step 8500:0.43689924797659035\n",
      "step 8600:0.43161716713146525\n",
      "\n",
      "test loss: 0.406463798994\n",
      "step 8700:0.4367515052967411\n",
      "step 8800:0.41841419540579144\n",
      "\n",
      "test loss: 0.421161318918\n",
      "step 8900:0.4226847334900061\n",
      "step 9000:0.42081495048679013\n",
      "\n",
      "test loss: 0.413301084469\n",
      "step 9100:0.42920384230330055\n",
      "step 9200:0.4186855541220195\n",
      "\n",
      "test loss: 0.413778341573\n",
      "step 9300:0.40991419634081633\n",
      "step 9400:0.416647907480428\n",
      "\n",
      "test loss: 0.413038403696\n",
      "step 9500:0.4106016150843268\n",
      "step 9600:0.4041915911875922\n",
      "\n",
      "test loss: 0.469745308022\n",
      "step 9700:0.3976705407216907\n",
      "step 9800:0.4010064391869731\n",
      "\n",
      "test loss: 0.422856668261\n",
      "step 9900:0.399564090963349\n",
      "step 10000:0.3949561536120103\n",
      "\n",
      "test loss: 0.745022459878\n",
      "step 10100:0.3952099360305465\n",
      "step 10200:0.3883332625335375\n",
      "\n",
      "test loss: 0.451862146467\n",
      "step 10300:0.3871973504559106\n",
      "step 10400:0.3903167595747937\n",
      "\n",
      "test loss: 0.472291577302\n",
      "step 10500:0.3678872427486792\n",
      "step 10600:0.3738556429214172\n",
      "\n",
      "test loss: 0.474301900077\n",
      "step 10700:0.3685419762987709\n",
      "step 10800:0.37134228130398517\n",
      "\n",
      "test loss: 0.608942066996\n",
      "step 10900:0.36568323921193374\n",
      "step 11000:0.3691976241058397\n",
      "\n",
      "test loss: 0.531840697367\n",
      "step 11100:0.358789317633215\n",
      "step 11200:0.3551924019427229\n",
      "\n",
      "test loss: 0.517651963719\n",
      "step 11300:0.34950455209316783\n",
      "step 11400:0.3524779531331627\n",
      "\n",
      "test loss: 0.578913151571\n",
      "step 11500:0.34223070690372304\n",
      "step 11600:0.33810842677377\n",
      "\n",
      "test loss: 0.580103085908\n",
      "step 11700:0.3361805069050829\n",
      "step 11800:0.32365032712941444\n",
      "\n",
      "test loss: 0.585804844459\n",
      "step 11900:0.3309058174138944\n",
      "step 12000:0.3181655879831088\n",
      "\n",
      "test loss: 0.721321996221\n",
      "step 12100:0.32183049871498465\n",
      "step 12200:0.30699623828120887\n",
      "\n",
      "test loss: 0.698171939517\n",
      "step 12300:0.29994269876322893\n",
      "step 12400:0.3016987351610477\n",
      "\n",
      "test loss: 0.7009821596\n",
      "step 12500:0.3004159669997409\n",
      "step 12600:0.2940452404752036\n",
      "\n",
      "test loss: 0.717071150913\n",
      "step 12700:0.29553557100366173\n",
      "step 12800:0.29126583777517423\n",
      "\n",
      "test loss: 0.735456141178\n",
      "step 12900:0.2840310162013779\n",
      "step 13000:0.2787801205711912\n",
      "\n",
      "test loss: 0.729694695814\n",
      "step 13100:0.2797064908655041\n",
      "step 13200:0.26873820879438415\n",
      "\n",
      "test loss: 0.774571145207\n",
      "step 13300:0.2692922527599619\n",
      "step 13400:0.2696489525329731\n",
      "\n",
      "test loss: 0.890203231446\n",
      "step 13500:0.2552301663066412\n",
      "step 13600:0.2562064251143695\n",
      "\n",
      "test loss: 0.819695720443\n",
      "step 13700:0.25492060246992787\n",
      "step 13800:0.25412735917552115\n",
      "\n",
      "test loss: 0.814761640275\n",
      "step 13900:0.24417700811438883\n",
      "step 14000:0.25000163364586214\n",
      "\n",
      "test loss: 0.84802231147\n",
      "step 14100:0.24544668678810144\n",
      "step 14200:0.2327081896999506\n",
      "\n",
      "test loss: 0.856847641059\n",
      "step 14300:0.23883842322226861\n",
      "step 14400:0.23197743023916761\n",
      "\n",
      "test loss: 0.846246352692\n",
      "step 14500:0.24201257473852675\n",
      "step 14600:0.23163613907622296\n",
      "\n",
      "test loss: 0.865555796995\n",
      "step 14700:0.23622774101929947\n",
      "step 14800:0.22366353830818092\n",
      "\n",
      "test loss: 0.939223603577\n",
      "step 14900:0.22037649272320828\n",
      "step 15000:0.22797567309443578\n",
      "\n",
      "test loss: 0.892992205567\n",
      "step 15100:0.21608712506157254\n",
      "step 15200:0.21888729225968373\n",
      "\n",
      "test loss: 0.909097618691\n",
      "step 15300:0.22061733568184444\n",
      "step 15400:0.2202407879663359\n",
      "\n",
      "test loss: 0.88248534618\n",
      "step 15500:0.21256799497421847\n",
      "step 15600:0.21262791148449728\n",
      "\n",
      "test loss: 0.856026168925\n",
      "step 15700:0.21255061317845078\n",
      "step 15800:0.20622418105234844\n",
      "\n",
      "test loss: 0.815487676282\n",
      "step 15900:0.20562719869789683\n",
      "step 16000:0.20338981042666537\n",
      "\n",
      "test loss: 0.786755955425\n",
      "step 16100:0.20962159375052059\n",
      "step 16200:0.19934847455412338\n",
      "\n",
      "test loss: 0.786618790995\n",
      "step 16300:0.20684597215708472\n",
      "step 16400:0.20532411118861954\n",
      "\n",
      "test loss: 0.774736074452\n",
      "step 16500:0.20956133509714805\n",
      "step 16600:0.20576946164348747\n",
      "\n",
      "test loss: 0.707033865201\n",
      "step 16700:0.20199914815497064\n",
      "step 16800:0.19945243244685162\n",
      "\n",
      "test loss: 0.716736039466\n",
      "step 16900:0.2030036416688355\n",
      "step 17000:0.19629679194204852\n",
      "\n",
      "test loss: 0.666850782329\n",
      "step 17100:0.19284925004086223\n",
      "step 17200:0.19907160893578577\n",
      "\n",
      "test loss: 0.670114048597\n",
      "step 17300:0.19723925080689927\n",
      "step 17400:0.2052262113500335\n",
      "\n",
      "test loss: 0.636504960465\n",
      "step 17500:0.1901631440808082\n",
      "step 17600:0.19799894033710982\n",
      "\n",
      "test loss: 0.597135093862\n",
      "step 17700:0.19354429707804827\n",
      "step 17800:0.189985558627586\n",
      "\n",
      "test loss: 0.587510497552\n",
      "step 17900:0.1873491484687078\n",
      "step 18000:0.18880234866298032\n",
      "\n",
      "test loss: 0.566066204843\n",
      "step 18100:0.19933548152154015\n",
      "step 18200:0.1928042966182081\n",
      "\n",
      "test loss: 0.548209169464\n",
      "step 18300:0.19124228531852472\n",
      "step 18400:0.1920566767506037\n",
      "\n",
      "test loss: 0.521766880335\n",
      "step 18500:0.18568194383837217\n",
      "step 18600:0.1918904996545006\n",
      "\n",
      "test loss: 0.533890819827\n",
      "step 18700:0.1915848440497698\n",
      "step 18800:0.18688702658223902\n",
      "\n",
      "test loss: 0.505225811439\n",
      "step 18900:0.18742771339537637\n",
      "step 19000:0.19226945420733602\n",
      "\n",
      "test loss: 0.522143841194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19100:0.18810997518684333\n",
      "step 19200:0.18418867453542134\n",
      "\n",
      "test loss: 0.49413294192\n",
      "step 19300:0.18898592566324174\n",
      "step 19400:0.18394207054643918\n",
      "\n",
      "test loss: 0.459178172539\n",
      "step 19500:0.18069908828372058\n",
      "step 19600:0.18955971253477594\n",
      "\n",
      "test loss: 0.448245445659\n",
      "step 19700:0.18662268089311881\n",
      "step 19800:0.1838757477246177\n",
      "\n",
      "test loss: 0.43415642432\n",
      "step 19900:0.17677190413103688\n",
      "step 20000:0.18336482052830333\n",
      "\n",
      "test loss: 0.431309829129\n",
      "step 20100:0.1772354996444064\n",
      "step 20200:0.17816607624724506\n",
      "\n",
      "test loss: 0.436954544077\n",
      "step 20300:0.18258894614957574\n",
      "step 20400:0.18117400117744445\n",
      "\n",
      "test loss: 0.417492739186\n",
      "step 20500:0.18712191020690344\n",
      "step 20600:0.18347649701364668\n",
      "\n",
      "test loss: 0.41987094453\n",
      "step 20700:0.18317328122753995\n",
      "step 20800:0.18270289905504145\n",
      "\n",
      "test loss: 0.385260343892\n",
      "step 20900:0.17823521789044555\n",
      "step 21000:0.17932563655690173\n",
      "\n",
      "test loss: 0.388760582882\n",
      "step 21100:0.17967010783157147\n",
      "step 21200:0.17762747161942843\n",
      "\n",
      "test loss: 0.374886642212\n",
      "step 21300:0.18611936876414278\n",
      "step 21400:0.17902335054527868\n",
      "\n",
      "test loss: 0.390735048955\n",
      "step 21500:0.1736565402351481\n",
      "step 21600:0.1733753545290433\n",
      "\n",
      "test loss: 0.364014421502\n",
      "step 21700:0.17669956688526547\n",
      "step 21800:0.16885677448903022\n",
      "\n",
      "test loss: 0.356090840558\n",
      "step 21900:0.17106062737245029\n",
      "step 22000:0.1724509141923919\n",
      "\n",
      "test loss: 0.379157222043\n",
      "step 22100:0.17398645344550712\n",
      "step 22200:0.17476286053312914\n",
      "\n",
      "test loss: 0.391431833059\n",
      "step 22300:0.1726520757746543\n",
      "step 22400:0.1722698564113497\n",
      "\n",
      "test loss: 0.353696381422\n",
      "step 22500:0.17054282282055014\n",
      "step 22600:0.17110938055821495\n",
      "\n",
      "test loss: 0.345554579159\n",
      "step 22700:0.1734684760599179\n",
      "step 22800:0.17438362130274224\n",
      "\n",
      "test loss: 0.325093253244\n",
      "step 22900:0.17563225956099418\n",
      "step 23000:0.17005754609874052\n",
      "\n",
      "test loss: 0.345634392286\n",
      "step 23100:0.17501650939431737\n",
      "step 23200:0.1696442023057125\n",
      "\n",
      "test loss: 0.35268211068\n",
      "step 23300:0.175230841292553\n",
      "step 23400:0.16515373908473535\n",
      "\n",
      "test loss: 0.374443256415\n",
      "step 23500:0.16937830507016108\n",
      "step 23600:0.1690816420664135\n",
      "\n",
      "test loss: 0.32128633078\n",
      "step 23700:0.16635616081552365\n",
      "step 23800:0.16859973144189566\n",
      "\n",
      "test loss: 0.399715950805\n",
      "step 23900:0.17311770843969912\n",
      "step 24000:0.16939342365241206\n",
      "\n",
      "test loss: 0.322682918848\n",
      "step 24100:0.1686538332047064\n",
      "step 24200:0.1712849108407603\n",
      "\n",
      "test loss: 0.287354862724\n",
      "step 24300:0.16988681537192835\n",
      "step 24400:0.1717567955154755\n",
      "\n",
      "test loss: 0.292635603398\n",
      "step 24500:0.17395439354690376\n",
      "step 24600:0.16576955193838164\n",
      "\n",
      "test loss: 0.287026016629\n",
      "step 24700:0.1655644033907403\n",
      "step 24800:0.17052307603159558\n",
      "\n",
      "test loss: 0.278134686215\n",
      "step 24900:0.16116416994000257\n",
      "step 25000:0.16575844296606956\n",
      "\n",
      "test loss: 0.263460518756\n",
      "step 25100:0.16386205290038067\n",
      "step 25200:0.1625711045847212\n",
      "\n",
      "test loss: 0.259506969395\n",
      "step 25300:0.16277117845701514\n",
      "step 25400:0.16075817875714338\n",
      "\n",
      "test loss: 0.248388634933\n",
      "step 25500:0.16001258196922502\n",
      "step 25600:0.16613763172591012\n",
      "\n",
      "test loss: 0.244535727385\n",
      "step 25700:0.16641678443634605\n",
      "step 25800:0.15974825776865867\n",
      "\n",
      "test loss: 0.254109348649\n",
      "step 25900:0.1646160996919259\n",
      "step 26000:0.16047003228972223\n",
      "\n",
      "test loss: 0.233320556176\n",
      "step 26100:0.16569874138665022\n",
      "step 26200:0.16335260587790637\n",
      "\n",
      "test loss: 0.215948459031\n",
      "step 26300:0.15835591211522576\n",
      "step 26400:0.15844590591737004\n",
      "\n",
      "test loss: 0.21661985473\n",
      "step 26500:0.15842347134521523\n",
      "step 26600:0.15545511767311226\n",
      "\n",
      "test loss: 0.21871907922\n",
      "step 26700:0.1615807964748652\n",
      "step 26800:0.15587014604368069\n",
      "\n",
      "test loss: 0.211462705079\n",
      "step 26900:0.16040135375972403\n",
      "step 27000:0.1636487498562635\n",
      "\n",
      "test loss: 0.20104024464\n",
      "step 27100:0.16250024197874494\n",
      "step 27200:0.1584661458078733\n",
      "\n",
      "test loss: 0.197180525311\n",
      "step 27300:0.1619688431568518\n",
      "step 27400:0.15481602998083907\n",
      "\n",
      "test loss: 0.200339454747\n",
      "step 27500:0.15768363335391117\n",
      "step 27600:0.15498955238181097\n",
      "\n",
      "test loss: 0.222706083051\n",
      "step 27700:0.16081297333929245\n",
      "step 27800:0.15797047705296532\n",
      "\n",
      "test loss: 0.270609661564\n",
      "step 27900:0.15575003558544176\n",
      "step 28000:0.15629486781281063\n",
      "\n",
      "test loss: 0.220841742494\n",
      "step 28100:0.15736146556734962\n",
      "step 28200:0.15943832917447243\n",
      "\n",
      "test loss: 0.198539049206\n",
      "step 28300:0.1531177715653957\n",
      "step 28400:0.155123026322384\n",
      "\n",
      "test loss: 0.227314028788\n",
      "step 28500:0.1518957828491804\n",
      "step 28600:0.1537273768470076\n",
      "\n",
      "test loss: 0.189160336113\n",
      "step 28700:0.15031119066810075\n",
      "step 28800:0.14795628599962143\n",
      "\n",
      "test loss: 0.202315300955\n",
      "step 28900:0.15077461390919067\n",
      "step 29000:0.14795523962462614\n",
      "\n",
      "test loss: 0.212165938942\n",
      "step 29100:0.15100949004858866\n",
      "step 29200:0.15279452982290187\n",
      "\n",
      "test loss: 0.180577998465\n",
      "step 29300:0.1518609466852077\n",
      "step 29400:0.1505145771435519\n",
      "\n",
      "test loss: 0.181535136161\n",
      "step 29500:0.1491444784973181\n",
      "step 29600:0.15318310715863157\n",
      "\n",
      "test loss: 0.181018172144\n",
      "step 29700:0.14369725233136327\n",
      "step 29800:0.14668909274722866\n",
      "\n",
      "test loss: 0.181864804759\n",
      "step 29900:0.14870334925842832\n",
      "step 30000:0.14525557296319047\n",
      "\n",
      "test loss: 0.181203195904\n",
      "step 30100:0.14886262456607605\n",
      "step 30200:0.14388245126462956\n",
      "\n",
      "test loss: 0.17688539393\n",
      "step 30300:0.15027030335138328\n",
      "step 30400:0.15149133211669397\n",
      "\n",
      "test loss: 0.164947698285\n",
      "step 30500:0.145758942813185\n",
      "step 30600:0.14641604849673392\n",
      "\n",
      "test loss: 0.189877284234\n",
      "step 30700:0.14568886298655176\n",
      "step 30800:0.14731494735736056\n",
      "\n",
      "test loss: 0.168782990513\n",
      "step 30900:0.14720692243682393\n",
      "step 31000:0.14539196915523553\n",
      "\n",
      "test loss: 0.165094787154\n",
      "step 31100:0.14663533987174984\n",
      "step 31200:0.15307671520086683\n",
      "\n",
      "test loss: 0.160749063257\n",
      "step 31300:0.1413580878884617\n",
      "step 31400:0.14251920680834645\n",
      "\n",
      "test loss: 0.625776024158\n",
      "step 31500:0.15225264239678446\n",
      "step 31600:0.14777315250528017\n",
      "\n",
      "test loss: 0.296183574318\n",
      "step 31700:0.14873955943686074\n",
      "step 31800:0.14686316824830797\n",
      "\n",
      "test loss: 0.153886852771\n",
      "step 31900:0.1447797429262422\n",
      "step 32000:0.14731326223846558\n",
      "\n",
      "test loss: 0.170069750387\n",
      "step 32100:0.14691998009848586\n",
      "step 32200:0.14373036641229164\n",
      "\n",
      "test loss: 0.166526694479\n",
      "step 32300:0.14716308160955885\n",
      "step 32400:0.14198761945761165\n",
      "\n",
      "test loss: 0.148529661991\n",
      "step 32500:0.14527253616105307\n",
      "step 32600:0.1438953297429421\n",
      "\n",
      "test loss: 0.142080420263\n",
      "step 32700:0.1465563278261062\n",
      "step 32800:0.14531970485154597\n",
      "\n",
      "test loss: 0.136777048253\n",
      "step 32900:0.14202118097272745\n",
      "step 33000:0.14982817929723938\n",
      "\n",
      "test loss: 0.14243727479\n",
      "step 33100:0.13869041565257434\n",
      "step 33200:0.14322069884812466\n",
      "\n",
      "test loss: 0.151231253023\n",
      "step 33300:0.14252988311465403\n",
      "step 33400:0.14282799905974197\n",
      "\n",
      "test loss: 0.141451287155\n",
      "step 33500:0.14192875593017756\n",
      "step 33600:0.14275080506516144\n",
      "\n",
      "test loss: 0.147400426803\n",
      "step 33700:0.1414688274181961\n",
      "step 33800:0.13828878994666516\n",
      "\n",
      "test loss: 0.157446149625\n",
      "step 33900:0.1417547132280743\n",
      "step 34000:0.13854630589718042\n",
      "\n",
      "test loss: 0.149861696384\n",
      "step 34100:0.1389776745997509\n",
      "step 34200:0.14106284151641033\n",
      "\n",
      "test loss: 0.146840642379\n",
      "step 34300:0.14456290051929754\n",
      "step 34400:0.13816906772956997\n",
      "\n",
      "test loss: 0.132519512776\n",
      "step 34500:0.1396099301525163\n",
      "step 34600:0.14020630179102525\n",
      "\n",
      "test loss: 0.132445032118\n",
      "step 34700:0.1371891471997509\n",
      "step 34800:0.13361357640056454\n",
      "\n",
      "test loss: 0.128229977886\n",
      "step 34900:0.14140978705131108\n",
      "step 35000:0.13392625955324672\n",
      "\n",
      "test loss: 0.176180921179\n",
      "step 35100:0.13617852788235285\n",
      "step 35200:0.13939787666311834\n",
      "\n",
      "test loss: 0.137648910113\n",
      "step 35300:0.13604989802711856\n",
      "step 35400:0.13105997918478998\n",
      "\n",
      "test loss: 0.155682754621\n",
      "step 35500:0.1312357504838853\n",
      "step 35600:0.13980679828988407\n",
      "\n",
      "test loss: 0.145760723916\n",
      "step 35700:0.12766139241151742\n",
      "step 35800:0.1326260453544005\n",
      "\n",
      "test loss: 0.141848535009\n",
      "step 35900:0.13211375888786375\n",
      "step 36000:0.13689822694089454\n",
      "\n",
      "test loss: 0.149976970876\n",
      "step 36100:0.13406474237942548\n",
      "step 36200:0.13308220408715563\n",
      "\n",
      "test loss: 0.123236153114\n",
      "step 36300:0.13474260551489395\n",
      "step 36400:0.13573812743884284\n",
      "\n",
      "test loss: 0.123198197102\n",
      "step 36500:0.12816681318334827\n",
      "step 36600:0.13309906984589837\n",
      "\n",
      "test loss: 0.125948707673\n",
      "step 36700:0.1345893850331204\n",
      "step 36800:0.1346737489445063\n",
      "\n",
      "test loss: 0.152956688785\n",
      "step 36900:0.13069644450133205\n",
      "step 37000:0.12701406258320896\n",
      "\n",
      "test loss: 0.126006008993\n",
      "step 37100:0.13320216044898084\n",
      "step 37200:0.12886043377243456\n",
      "\n",
      "test loss: 0.204016329826\n",
      "step 37300:0.13277681454697554\n",
      "step 37400:0.12748619039201428\n",
      "\n",
      "test loss: 0.173071217343\n",
      "step 37500:0.1333282321255233\n",
      "step 37600:0.1289861578875999\n",
      "\n",
      "test loss: 0.110523244788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 37700:0.13294293950162314\n",
      "step 37800:0.13348740689042946\n",
      "\n",
      "test loss: 0.112885927097\n",
      "step 37900:0.12798504961639218\n",
      "step 38000:0.13124831041130242\n",
      "\n",
      "test loss: 0.14322003864\n",
      "step 38100:0.13300106148792804\n",
      "step 38200:0.13232775579768788\n",
      "\n",
      "test loss: 0.123619770887\n",
      "step 38300:0.1307816875416424\n",
      "step 38400:0.13177268449968113\n",
      "\n",
      "test loss: 0.103131399939\n",
      "step 38500:0.12638583762080777\n",
      "step 38600:0.12577753442399764\n",
      "\n",
      "test loss: 0.110003927632\n",
      "step 38700:0.1300913201361569\n",
      "step 38800:0.1274305785670005\n",
      "\n",
      "test loss: 0.106283638737\n",
      "step 38900:0.12089251798970084\n",
      "step 39000:0.1271761969246486\n",
      "\n",
      "test loss: 0.113294899987\n",
      "step 39100:0.13175612425346375\n",
      "step 39200:0.1287938637331849\n",
      "\n",
      "test loss: 0.121365075171\n",
      "step 39300:0.12515082303245437\n",
      "step 39400:0.12467654329487363\n",
      "\n",
      "test loss: 0.101236925947\n",
      "step 39500:0.12881086265697403\n",
      "step 39600:0.1254243144626434\n",
      "\n",
      "test loss: 0.135277375859\n",
      "step 39700:0.123838693936212\n",
      "step 39800:0.1229222266416718\n",
      "\n",
      "test loss: 0.097664670215\n",
      "step 39900:0.1259389815370136\n",
      "step 40000:0.1258776421141613\n",
      "\n",
      "test loss: 0.0980034281914\n",
      "step 40100:0.12316554890328865\n",
      "step 40200:0.1279593707167617\n",
      "\n",
      "test loss: 0.127112032847\n",
      "step 40300:0.12555927193540106\n",
      "step 40400:0.1232288722746303\n",
      "\n",
      "test loss: 0.118039588914\n",
      "step 40500:0.12609780866906592\n",
      "step 40600:0.12836652194494713\n",
      "\n",
      "test loss: 0.100625725483\n",
      "step 40700:0.12606183744370417\n",
      "step 40800:0.12141774766084468\n",
      "\n",
      "test loss: 0.112131991756\n",
      "step 40900:0.12845442518015424\n",
      "step 41000:0.13014674224109404\n",
      "\n",
      "test loss: 0.0935288313338\n",
      "step 41100:0.12406126219116469\n",
      "step 41200:0.12302827132319855\n",
      "\n",
      "test loss: 0.151629624375\n",
      "step 41300:0.12254340382523085\n",
      "step 41400:0.1220265562097635\n",
      "\n",
      "test loss: 0.141763026976\n",
      "step 41500:0.12530042644751807\n",
      "step 41600:0.1256583066442341\n",
      "\n",
      "test loss: 0.107686164877\n",
      "step 41700:0.12602762758552868\n",
      "step 41800:0.12079424878997523\n",
      "\n",
      "test loss: 0.155226205315\n",
      "step 41900:0.12610976364995635\n",
      "step 42000:0.1238007474053847\n",
      "\n",
      "test loss: 0.140395046289\n",
      "step 42100:0.12001339739353269\n",
      "step 42200:0.12088084666655434\n",
      "\n",
      "test loss: 0.0929666411191\n",
      "step 42300:0.12522971416751855\n",
      "step 42400:0.11859256348149895\n",
      "\n",
      "test loss: 0.105299329683\n",
      "step 42500:0.12348578250142954\n",
      "step 42600:0.11803290214394843\n",
      "\n",
      "test loss: 0.109701324727\n",
      "step 42700:0.11922831313912381\n",
      "step 42800:0.11944632211577967\n",
      "\n",
      "test loss: 0.101116216272\n",
      "step 42900:0.12384777467891313\n",
      "step 43000:0.1199064574765252\n",
      "\n",
      "test loss: 0.105658239081\n",
      "step 43100:0.1278880660242568\n",
      "step 43200:0.12063089911826273\n",
      "\n",
      "test loss: 0.102092878623\n",
      "step 43300:0.12338023080184833\n",
      "step 43400:0.1199716415432677\n",
      "\n",
      "test loss: 0.114500716987\n",
      "step 43500:0.11903658038645996\n",
      "step 43600:0.11681646643332852\n",
      "\n",
      "test loss: 0.11113801437\n",
      "step 43700:0.11693899954223778\n",
      "step 43800:0.1184511667588425\n",
      "\n",
      "test loss: 0.089206169626\n",
      "step 43900:0.11667912801038392\n",
      "step 44000:0.1149158270120026\n",
      "\n",
      "test loss: 0.0925802122242\n",
      "step 44100:0.12166033838028595\n",
      "step 44200:0.11435718765296618\n",
      "\n",
      "test loss: 0.0970536952375\n",
      "step 44300:0.11901486984780799\n",
      "step 44400:0.11593834420639991\n",
      "\n",
      "test loss: 0.104776638044\n",
      "step 44500:0.11382744529990906\n",
      "step 44600:0.11628833173906347\n",
      "\n",
      "test loss: 0.0872734420473\n",
      "step 44700:0.11586390766188354\n",
      "step 44800:0.11792084022937452\n",
      "\n",
      "test loss: 0.0932043997016\n",
      "step 44900:0.12609298485285614\n",
      "step 45000:0.1168674900623596\n",
      "\n",
      "test loss: 0.0933715844214\n",
      "step 45100:0.11619621488442335\n",
      "step 45200:0.11690294307164034\n",
      "\n",
      "test loss: 0.0870388096118\n",
      "step 45300:0.11435926615200187\n",
      "step 45400:0.11217712464296298\n",
      "\n",
      "test loss: 0.148936194978\n",
      "step 45500:0.11991254723272692\n",
      "step 45600:0.11425954778985424\n",
      "\n",
      "test loss: 0.0883663261224\n",
      "step 45700:0.11640024883099025\n",
      "step 45800:0.11342889788482696\n",
      "\n",
      "test loss: 0.108360223254\n",
      "step 45900:0.11724415729459274\n",
      "step 46000:0.11275601318498177\n",
      "\n",
      "test loss: 0.100094582345\n",
      "step 46100:0.11202266233706135\n",
      "step 46200:0.11412973195647434\n",
      "\n",
      "test loss: 0.110381874803\n",
      "step 46300:0.1152908511377987\n",
      "step 46400:0.11105008359480285\n",
      "\n",
      "test loss: 0.1704130401\n",
      "step 46500:0.11287828649502861\n",
      "step 46600:0.1097649247139598\n",
      "\n",
      "test loss: 0.181658241644\n",
      "step 46700:0.1122082914194449\n",
      "step 46800:0.1113673014828593\n",
      "\n",
      "test loss: 0.0927354174809\n",
      "step 46900:0.1143214759158865\n",
      "step 47000:0.112105341167552\n",
      "\n",
      "test loss: 0.0922750689855\n",
      "step 47100:0.11323674480245781\n",
      "step 47200:0.1137460309948771\n",
      "\n",
      "test loss: 0.0849273326605\n",
      "step 47300:0.11189187904946456\n",
      "step 47400:0.1125663378141656\n",
      "\n",
      "test loss: 0.0857999582494\n",
      "step 47500:0.11285189707238487\n",
      "step 47600:0.1127636242239215\n",
      "\n",
      "test loss: 0.0908757168804\n",
      "step 47700:0.10923425199828468\n",
      "step 47800:0.10545758605140618\n",
      "\n",
      "test loss: 0.0816282802602\n",
      "step 47900:0.11143107113129803\n",
      "step 48000:0.10753218047816124\n",
      "\n",
      "test loss: 0.101047861741\n",
      "step 48100:0.11342131862494863\n",
      "step 48200:0.10852581124141958\n",
      "\n",
      "test loss: 0.224887534969\n",
      "step 48300:0.11386888483616273\n",
      "step 48400:0.10968978894470398\n",
      "\n",
      "test loss: 0.165612674142\n",
      "step 48500:0.11424056369493478\n",
      "step 48600:0.10596937657267977\n",
      "\n",
      "test loss: 0.0869624022023\n",
      "step 48700:0.111678142939618\n",
      "step 48800:0.1066761345787172\n",
      "\n",
      "test loss: 0.0870364839212\n",
      "step 48900:0.11037456021794853\n",
      "step 49000:0.10328557666469611\n",
      "\n",
      "test loss: 0.0873722076996\n",
      "step 49100:0.10888146609889353\n",
      "step 49200:0.11142966810303485\n",
      "\n",
      "test loss: 0.0951659019036\n",
      "step 49300:0.10540906794560505\n",
      "step 49400:0.10703745075242942\n",
      "\n",
      "test loss: 0.0952489864622\n",
      "step 49500:0.10852863901012484\n",
      "step 49600:0.10630520032005943\n",
      "\n",
      "test loss: 0.146616220702\n",
      "step 49700:0.10501690454109866\n",
      "step 49800:0.10814360628249778\n",
      "\n",
      "test loss: 0.0886262744571\n",
      "step 49900:0.10770329984045789\n",
      "step 50000:0.1084287856389375\n",
      "\n",
      "test loss: 0.0850802653303\n",
      "step 50100:0.10858164292119098\n",
      "step 50200:0.10100927851674962\n",
      "\n",
      "test loss: 0.0861385865747\n",
      "step 50300:0.10397414269219116\n",
      "step 50400:0.10777714668105164\n",
      "\n",
      "test loss: 0.0867662532217\n",
      "step 50500:0.10517859034852023\n",
      "step 50600:0.1084808214727529\n",
      "\n",
      "test loss: 0.107661467028\n",
      "step 50700:0.1043649224546963\n",
      "step 50800:0.10596280731841685\n",
      "\n",
      "test loss: 0.0908188112027\n",
      "step 50900:0.10413027760691886\n",
      "step 51000:0.10591870408883694\n",
      "\n",
      "test loss: 0.0846931530875\n",
      "step 51100:0.10337539263320461\n",
      "step 51200:0.10704385314970939\n",
      "\n",
      "test loss: 0.0830769074188\n",
      "step 51300:0.10557286277651967\n",
      "step 51400:0.10608153267353754\n",
      "\n",
      "test loss: 0.0876368648909\n",
      "step 51500:0.10281466051643254\n",
      "step 51600:0.10280295777055433\n",
      "\n",
      "test loss: 0.0891588334893\n",
      "step 51700:0.10439825425266923\n",
      "step 51800:0.10669837145448076\n",
      "\n",
      "test loss: 0.0862451123094\n",
      "step 51900:0.10239492014299921\n",
      "step 52000:0.10441904030034864\n",
      "\n",
      "test loss: 0.0903679610689\n",
      "step 52100:0.10606896890033304\n",
      "step 52200:0.10138649034963833\n",
      "\n",
      "test loss: 0.101248570293\n",
      "step 52300:0.10606081016493431\n",
      "step 52400:0.11279610892119081\n",
      "\n",
      "test loss: 0.0828169876182\n",
      "step 52500:0.10198786610322956\n",
      "step 52600:0.10472001846355675\n",
      "\n",
      "test loss: 0.082126737038\n",
      "step 52700:0.10448576404905434\n",
      "step 52800:0.10455889256437667\n",
      "\n",
      "test loss: 0.0856021205265\n",
      "step 52900:0.1002935298995095\n",
      "step 53000:0.10549626839069717\n",
      "\n",
      "test loss: 0.0914604395984\n",
      "step 53100:0.10653567454322668\n",
      "step 53200:0.10251556705161884\n",
      "\n",
      "test loss: 0.127074596569\n",
      "step 53300:0.10344803162507633\n",
      "step 53400:0.10126662739985508\n",
      "\n",
      "test loss: 0.113378248457\n",
      "step 53500:0.1025585315149057\n",
      "step 53600:0.10456975541346211\n",
      "\n",
      "test loss: 0.11012877298\n",
      "step 53700:0.10532069541871042\n",
      "step 53800:0.10252415323038003\n",
      "\n",
      "test loss: 0.136579918063\n",
      "step 53900:0.10495863290996287\n",
      "step 54000:0.09954474960047777\n",
      "\n",
      "test loss: 0.0851588350549\n",
      "step 54100:0.10162447568784079\n",
      "step 54200:0.10501096979300638\n",
      "\n",
      "test loss: 0.103624107387\n",
      "step 54300:0.09925394413910332\n",
      "step 54400:0.09971719627810739\n",
      "\n",
      "test loss: 0.0913951669445\n",
      "step 54500:0.10007344112301393\n",
      "step 54600:0.1018010799730118\n",
      "\n",
      "test loss: 0.110819077981\n",
      "step 54700:0.10367278098706208\n",
      "step 54800:0.10061306776855705\n",
      "\n",
      "test loss: 0.0870116635982\n",
      "step 54900:0.09893090448453767\n",
      "step 55000:0.09885796751058147\n",
      "\n",
      "test loss: 0.0836827728668\n",
      "step 55100:0.09894700521665777\n",
      "step 55200:0.10490806005001774\n",
      "\n",
      "test loss: 0.0786829531204\n",
      "step 55300:0.09881068190752154\n",
      "step 55400:0.09897116054365006\n",
      "\n",
      "test loss: 0.195646128564\n",
      "step 55500:0.09984912551182903\n",
      "step 55600:0.09793984278215294\n",
      "\n",
      "test loss: 0.160137844665\n",
      "step 55700:0.09659357510442469\n",
      "step 55800:0.10030304517286193\n",
      "\n",
      "test loss: 0.0964148469842\n",
      "step 55900:0.09825517086056706\n",
      "step 56000:0.09452418964862958\n",
      "\n",
      "test loss: 0.0875455976224\n",
      "step 56100:0.0990593673691427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 56200:0.09890948973487707\n",
      "\n",
      "test loss: 0.092258557482\n",
      "step 56300:0.09749883091461996\n",
      "step 56400:0.10075027942634966\n",
      "\n",
      "test loss: 0.0868314555163\n",
      "step 56500:0.0971803248400361\n",
      "step 56600:0.09582115828124413\n",
      "\n",
      "test loss: 0.158323990003\n",
      "step 56700:0.10136459843393783\n",
      "step 56800:0.09790443373013583\n",
      "\n",
      "test loss: 0.0855492574299\n",
      "step 56900:0.09953980171692264\n",
      "step 57000:0.09704331064035866\n",
      "\n",
      "test loss: 0.0919386628769\n",
      "step 57100:0.09699798480794888\n",
      "step 57200:0.09885399680150665\n",
      "\n",
      "test loss: 0.0945650572036\n",
      "step 57300:0.09930581732081063\n",
      "step 57400:0.09726103690489012\n",
      "\n",
      "test loss: 0.079585536753\n",
      "step 57500:0.09728608462310535\n",
      "step 57600:0.09534102345151657\n",
      "\n",
      "test loss: 0.0819548997313\n",
      "step 57700:0.09635274453934001\n",
      "step 57800:0.09943431724119607\n",
      "\n",
      "test loss: 0.0790821162157\n",
      "step 57900:0.09389327562609773\n",
      "step 58000:0.0993051557898135\n",
      "\n",
      "test loss: 0.083303884995\n",
      "step 58100:0.09679442044322661\n",
      "step 58200:0.09856291921439318\n",
      "\n",
      "test loss: 0.0774842105297\n",
      "step 58300:0.09972792111440366\n",
      "step 58400:0.09624137770368535\n",
      "\n",
      "test loss: 0.0812500346853\n",
      "step 58500:0.09744627789491271\n",
      "step 58600:0.09665644263855097\n",
      "\n",
      "test loss: 0.0862650424019\n",
      "step 58700:0.09748653108819248\n",
      "step 58800:0.09481082706431816\n",
      "\n",
      "test loss: 0.0839531830413\n",
      "step 58900:0.09832473625107849\n",
      "step 59000:0.092639907495042\n",
      "\n",
      "test loss: 0.074702147516\n",
      "step 59100:0.09692391529684857\n",
      "step 59200:0.09196704823985154\n",
      "\n",
      "test loss: 0.0980276532763\n",
      "step 59300:0.09537383595012593\n",
      "step 59400:0.09371880603953135\n",
      "\n",
      "test loss: 0.0936706071824\n",
      "step 59500:0.09453950263690533\n",
      "step 59600:0.09790868328520246\n",
      "\n",
      "test loss: 0.081394900855\n",
      "step 59700:0.09543038777049576\n",
      "step 59800:0.09598438600087063\n",
      "\n",
      "test loss: 0.0909367091151\n",
      "step 59900:0.09382284551733351\n",
      "step 60000:0.09578380593449669\n",
      "\n",
      "test loss: 0.100524567042\n",
      "step 60100:0.09262779771957261\n",
      "step 60200:0.09770436757522245\n",
      "\n",
      "test loss: 0.0730295518795\n",
      "step 60300:0.09122990579369643\n",
      "step 60400:0.09596109302024738\n",
      "\n",
      "test loss: 0.0754797543128\n",
      "step 60500:0.0930008381721309\n",
      "step 60600:0.09395939927073785\n",
      "\n",
      "test loss: 0.0782638825492\n",
      "step 60700:0.09282976437360833\n",
      "step 60800:0.09279002843092439\n",
      "\n",
      "test loss: 0.095269591844\n",
      "step 60900:0.09390441983831059\n",
      "step 61000:0.09139620228336841\n",
      "\n",
      "test loss: 0.0853665348605\n",
      "step 61100:0.09084200071748116\n",
      "step 61200:0.09373488279863917\n",
      "\n",
      "test loss: 0.0985575137459\n",
      "step 61300:0.0896958920217999\n",
      "step 61400:0.09132083141762233\n",
      "\n",
      "test loss: 0.0925290967937\n",
      "step 61500:0.09226054764163263\n",
      "step 61600:0.09539131031422635\n",
      "\n",
      "test loss: 0.120512422504\n",
      "step 61700:0.09108159795588577\n",
      "step 61800:0.0909507303526221\n",
      "\n",
      "test loss: 0.140131489169\n",
      "step 61900:0.09654130384260862\n",
      "step 62000:0.08968228473087975\n",
      "\n",
      "test loss: 0.105981849991\n",
      "step 62100:0.0896491598922621\n",
      "step 62200:0.09290674031552959\n",
      "\n",
      "test loss: 0.0851002589211\n",
      "step 62300:0.09221992593752487\n",
      "step 62400:0.08886886866805371\n",
      "\n",
      "test loss: 0.0828473439176\n",
      "step 62500:0.09038492073491214\n",
      "step 62600:0.09067432113130859\n",
      "\n",
      "test loss: 0.0716400413004\n",
      "step 62700:0.08868348528304527\n",
      "step 62800:0.09274543947486147\n",
      "\n",
      "test loss: 0.0689496192939\n",
      "step 62900:0.08822259449485811\n",
      "step 63000:0.09199595306079748\n",
      "\n",
      "test loss: 0.0684049913643\n",
      "step 63100:0.08938587313479797\n",
      "step 63200:0.09001395329888029\n",
      "\n",
      "test loss: 0.0790219624805\n",
      "step 63300:0.09226077018128992\n",
      "step 63400:0.08599885576757642\n",
      "\n",
      "test loss: 0.0817762958266\n",
      "step 63500:0.08932345509873908\n",
      "step 63600:0.08651018670737487\n",
      "\n",
      "test loss: 0.0743593051345\n",
      "step 63700:0.08826252616493621\n",
      "step 63800:0.09148954379359944\n",
      "\n",
      "test loss: 0.0963244431552\n",
      "step 63900:0.09084968338893658\n",
      "step 64000:0.08971434747417073\n",
      "\n",
      "test loss: 0.0767711123217\n",
      "step 64100:0.08971969453996646\n",
      "step 64200:0.08641574059174786\n",
      "\n",
      "test loss: 0.0743228950996\n",
      "step 64300:0.0892486159781836\n",
      "step 64400:0.09146990494862368\n",
      "\n",
      "test loss: 0.0689081004525\n",
      "step 64500:0.08638737676770175\n",
      "step 64600:0.08885525238767801\n",
      "\n",
      "test loss: 0.0767057929459\n",
      "step 64700:0.08914435403230567\n",
      "step 64800:0.08997056514254421\n",
      "\n",
      "test loss: 0.0747985970161\n",
      "step 64900:0.08712936944843595\n",
      "step 65000:0.08851918765393722\n",
      "\n",
      "test loss: 0.0757248581169\n",
      "step 65100:0.08547913064130234\n",
      "step 65200:0.08732368279900031\n",
      "\n",
      "test loss: 0.0772976820309\n",
      "step 65300:0.08784845227440226\n",
      "step 65400:0.08622995796724119\n",
      "\n",
      "test loss: 0.0833633958927\n",
      "step 65500:0.08937182350879769\n",
      "step 65600:0.08774937414186158\n",
      "\n",
      "test loss: 0.0699836032816\n",
      "step 65700:0.08895785211522841\n",
      "step 65800:0.08667501494428267\n",
      "\n",
      "test loss: 0.0684241350383\n",
      "step 65900:0.08888136023371239\n",
      "step 66000:0.08640987196044318\n",
      "\n",
      "test loss: 0.0813100826529\n",
      "step 66100:0.08998742922875125\n",
      "step 66200:0.08719481371721745\n",
      "\n",
      "test loss: 0.0792743710892\n",
      "step 66300:0.08638649094211309\n",
      "step 66400:0.08524579928147702\n",
      "\n",
      "test loss: 0.0767494629452\n",
      "step 66500:0.08539876139224953\n",
      "step 66600:0.08672221369168602\n",
      "\n",
      "test loss: 0.0841458754003\n",
      "step 66700:0.0878482359334913\n",
      "step 66800:0.08661446761983524\n",
      "\n",
      "test loss: 0.0761621134168\n",
      "step 66900:0.08649093599325071\n",
      "step 67000:0.08712176835874906\n",
      "\n",
      "test loss: 0.132897452968\n",
      "step 67100:0.08357274488066217\n",
      "step 67200:0.08243916587178668\n",
      "\n",
      "test loss: 0.0854243519315\n",
      "step 67300:0.08433804652692023\n",
      "step 67400:0.08417387769791548\n",
      "\n",
      "test loss: 0.0879005412899\n",
      "step 67500:0.09005892873338221\n",
      "step 67600:0.08623481690174033\n",
      "\n",
      "test loss: 0.0871210200776\n",
      "step 67700:0.08830939603472596\n",
      "step 67800:0.08664091436793213\n",
      "\n",
      "test loss: 0.0703108132862\n",
      "step 67900:0.08725538528531131\n",
      "step 68000:0.08463153616753559\n",
      "\n",
      "test loss: 0.0692675666746\n",
      "step 68100:0.0858994323470956\n",
      "step 68200:0.08898630354894888\n",
      "\n",
      "test loss: 0.0664509912471\n",
      "step 68300:0.08190134470958525\n",
      "step 68400:0.08605713972236166\n",
      "\n",
      "test loss: 0.0894508412269\n",
      "step 68500:0.08413676717720885\n",
      "step 68600:0.08465462512577668\n",
      "\n",
      "test loss: 0.101969584234\n",
      "step 68700:0.08777757965554317\n",
      "step 68800:0.0844063804237482\n",
      "\n",
      "test loss: 0.114792430928\n",
      "step 68900:0.08615452846171459\n",
      "step 69000:0.08374061550439969\n",
      "\n",
      "test loss: 0.0690919911476\n",
      "step 69100:0.08568751752206638\n",
      "step 69200:0.08581295634894907\n",
      "\n",
      "test loss: 0.102887123552\n",
      "step 69300:0.08597864759889634\n",
      "step 69400:0.08601355973097106\n",
      "\n",
      "test loss: 0.141897437561\n",
      "step 69500:0.08248372846781077\n",
      "step 69600:0.08394173609431914\n",
      "\n",
      "test loss: 0.0639417792575\n",
      "step 69700:0.08289714173621084\n",
      "step 69800:0.08408431034898042\n",
      "\n",
      "test loss: 0.0666967250686\n",
      "step 69900:0.08248265230622122\n",
      "step 70000:0.0797086680254984\n",
      "\n",
      "test loss: 0.079361972685\n",
      "step 70100:0.08255821000870822\n",
      "step 70200:0.08356314117367487\n",
      "\n",
      "test loss: 0.0718334140544\n",
      "step 70300:0.08302760085607565\n",
      "step 70400:0.08348016791713066\n",
      "\n",
      "test loss: 0.0758179895495\n",
      "step 70500:0.08363234422707612\n",
      "step 70600:0.07882071905390801\n",
      "\n",
      "test loss: 0.0650565552164\n",
      "step 70700:0.08142814220785295\n",
      "step 70800:0.08404058832443413\n",
      "\n",
      "test loss: 0.0922323971436\n",
      "step 70900:0.08218175202832147\n",
      "step 71000:0.08283047474174962\n",
      "\n",
      "test loss: 0.0644940316235\n",
      "step 71100:0.08229572694305627\n",
      "step 71200:0.07870689368629152\n",
      "\n",
      "test loss: 0.0790715040848\n",
      "step 71300:0.0819667746613687\n",
      "step 71400:0.08008241467161381\n",
      "\n",
      "test loss: 0.0603730538052\n",
      "step 71500:0.08141095575970904\n",
      "step 71600:0.07886521605279116\n",
      "\n",
      "test loss: 0.212866548747\n",
      "step 71700:0.08756071962756103\n",
      "step 71800:0.0783109651727121\n",
      "\n",
      "test loss: 0.0719521179733\n",
      "step 71900:0.08169132689006924\n",
      "step 72000:0.08260630118912429\n",
      "\n",
      "test loss: 0.0706005733196\n",
      "step 72100:0.08065600085788846\n",
      "step 72200:0.08196933710542208\n",
      "\n",
      "test loss: 0.060337650438\n",
      "step 72300:0.08006841908803922\n",
      "step 72400:0.08020944260959446\n",
      "\n",
      "test loss: 0.0628133104845\n",
      "step 72500:0.08302586320174593\n",
      "step 72600:0.08123986613263823\n",
      "\n",
      "test loss: 0.070941112197\n",
      "step 72700:0.08135361646401582\n",
      "step 72800:0.08014243996165121\n",
      "\n",
      "test loss: 0.065414031255\n",
      "step 72900:0.08088747216562642\n",
      "step 73000:0.08179242507299156\n",
      "\n",
      "test loss: 0.06054845102\n",
      "step 73100:0.08213500678905451\n",
      "step 73200:0.07988160570443267\n",
      "\n",
      "test loss: 0.0620355776204\n",
      "step 73300:0.08083567011181271\n",
      "step 73400:0.08055025935551235\n",
      "\n",
      "test loss: 0.0611173307359\n",
      "step 73500:0.08091709456829267\n",
      "step 73600:0.07836755893948184\n",
      "\n",
      "test loss: 0.0621150609249\n",
      "step 73700:0.07859300923077088\n",
      "step 73800:0.08002719728032076\n",
      "\n",
      "test loss: 0.0659553700702\n",
      "step 73900:0.07979207458813489\n",
      "step 74000:0.08032989324275612\n",
      "\n",
      "test loss: 0.139469605018\n",
      "step 74100:0.08326983552073763\n",
      "step 74200:0.07930315371185803\n",
      "\n",
      "test loss: 0.0656133242525\n",
      "step 74300:0.08063181717146568\n",
      "step 74400:0.08011610193072605\n",
      "\n",
      "test loss: 0.0746700234823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 74500:0.07783802063346633\n",
      "step 74600:0.07791482443825754\n",
      "\n",
      "test loss: 0.0760329038644\n",
      "step 74700:0.08035781645568446\n",
      "step 74800:0.07830864175612551\n",
      "\n",
      "test loss: 0.0623673560109\n",
      "step 74900:0.07774510686690998\n",
      "step 75000:0.07949103983784155\n",
      "\n",
      "test loss: 0.0834902004386\n",
      "step 75100:0.08138161185825238\n",
      "step 75200:0.07605820064371713\n",
      "\n",
      "test loss: 0.123226199407\n",
      "step 75300:0.08018743474773647\n",
      "step 75400:0.07792082446897303\n",
      "\n",
      "test loss: 0.0793072601706\n",
      "step 75500:0.07672443976566597\n",
      "step 75600:0.07720689682303916\n",
      "\n",
      "test loss: 0.0591985687193\n",
      "step 75700:0.07397064229434717\n",
      "step 75800:0.07536465087262553\n",
      "\n",
      "test loss: 0.0621714247216\n",
      "step 75900:0.07652559956495911\n",
      "step 76000:0.07628918574958314\n",
      "\n",
      "test loss: 0.0628832150232\n",
      "step 76100:0.076136774152928\n",
      "step 76200:0.07391100607411474\n",
      "\n",
      "test loss: 0.104925310537\n",
      "step 76300:0.0806892281913222\n",
      "step 76400:0.07593734078348348\n",
      "\n",
      "test loss: 0.115173163153\n",
      "step 76500:0.07895618215932357\n",
      "step 76600:0.0805381828600303\n",
      "\n",
      "test loss: 0.231781265553\n",
      "step 76700:0.07708635291095256\n",
      "step 76800:0.07584851885263341\n",
      "\n",
      "test loss: 0.0601614909134\n",
      "step 76900:0.07722210232639809\n",
      "step 77000:0.07282375254245269\n",
      "\n",
      "test loss: 0.0694370366174\n",
      "step 77100:0.07640900262312501\n",
      "step 77200:0.07774331739133987\n",
      "\n",
      "test loss: 0.0669996391592\n",
      "step 77300:0.07367056702850193\n",
      "step 77400:0.07453847641037908\n",
      "\n",
      "test loss: 0.0842986096589\n",
      "step 77500:0.07415182102323777\n",
      "step 77600:0.07510851243801595\n",
      "\n",
      "test loss: 0.0889912049645\n",
      "step 77700:0.07540891244665575\n",
      "step 77800:0.07522697119180535\n",
      "\n",
      "test loss: 0.132019748389\n",
      "step 77900:0.07186632230375742\n",
      "step 78000:0.07460589881047758\n",
      "\n",
      "test loss: 0.0566919724674\n",
      "step 78100:0.07302529254792361\n",
      "step 78200:0.07328789939131146\n",
      "\n",
      "test loss: 0.0576755810552\n",
      "step 78300:0.07550301205350954\n",
      "step 78400:0.07498010854295446\n",
      "\n",
      "test loss: 0.0563854413741\n",
      "step 78500:0.07267976819520501\n",
      "step 78600:0.07289273816803542\n",
      "\n",
      "test loss: 0.0553198722128\n",
      "step 78700:0.07378256830538334\n",
      "step 78800:0.07258889005188955\n",
      "\n",
      "test loss: 0.0568436690063\n",
      "step 78900:0.072537345522373\n",
      "step 79000:0.07334406906062124\n",
      "\n",
      "test loss: 0.0620700072673\n",
      "step 79100:0.07305572665883721\n",
      "step 79200:0.07154930372532693\n",
      "\n",
      "test loss: 0.0598181681454\n",
      "step 79300:0.07194367827477717\n",
      "step 79400:0.07177448946082932\n",
      "\n",
      "test loss: 0.0697216468988\n",
      "step 79500:0.0709326611055551\n",
      "step 79600:0.07182111359502409\n",
      "\n",
      "test loss: 0.0572388248033\n",
      "step 79700:0.06891267156874978\n",
      "step 79800:0.069408089757421\n",
      "\n",
      "test loss: 0.0641009882269\n",
      "step 79900:0.07172329132036394\n",
      "step 80000:0.07185051333196696\n",
      "\n",
      "test loss: 0.0588812782273\n",
      "step 80100:0.06880146304435461\n",
      "step 80200:0.07232885157583853\n",
      "\n",
      "test loss: 0.0566626603308\n",
      "step 80300:0.07196258230867503\n",
      "step 80400:0.07342065318345645\n",
      "\n",
      "test loss: 0.0641991957695\n",
      "step 80500:0.07183734350304599\n",
      "step 80600:0.06925742396741036\n",
      "\n",
      "test loss: 0.0633302869231\n",
      "step 80700:0.07068198849347257\n",
      "step 80800:0.07058169954349394\n",
      "\n",
      "test loss: 0.0596155415353\n",
      "step 80900:0.07065936292962954\n",
      "step 81000:0.06925322644530865\n",
      "\n",
      "test loss: 0.0594396361258\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#saver.restore(sess, tf.train.latest_checkpoint('checkpoint/'))\n",
    "\n",
    "log_dir = './logs'\n",
    "\n",
    "train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "\n",
    "for i in range(10000000):\n",
    "    _, loss, train_summary = sess.run([train_step, cross_entropy, merged], feed_dict={X: trainingFeatures, y_:trainingLabels, dropout_keep_prob:0.3})\n",
    "    train_writer.add_summary(train_summary, i)\n",
    "    if i%100 == 0:\n",
    "        saver.save(sess, save_path='checkpoint/lin_classifier', global_step=i)\n",
    "        print('step {}:{}'.format(i, loss))\n",
    "    if i%200 == 0:\n",
    "        _, test_loss, test_summary = sess.run([train_step, cross_entropy, merged], feed_dict={X: testFeatures, y_:testLabels, dropout_keep_prob:1.0})\n",
    "        test_writer.add_summary(test_summary, i)\n",
    "        print('\\ntest loss:', test_loss)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
