{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input00 = open(\"input00.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(input00)\n",
    "numTrainingData, numFeatures = (int(s) for s in input00[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create numpy arrays to store training features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trainingFeatures = np.zeros(shape=(numTrainingData, numFeatures))\n",
    "trainingLabels = np.zeros(shape=(numTrainingData,), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFeatures = input00[1:numTrainingData+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean training dataset, remove unnecessary characters and store in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for entryNum in range(trainingFeatures.shape[0]):\n",
    "    entry = inputFeatures[entryNum].split()\n",
    "    #print(entry)\n",
    "    trainingLabels[entryNum] = float(entry[1])\n",
    "    params = [entry[i] for i in range(2, numFeatures + 2)]\n",
    "    cleanedParams = [param[param.index(\":\") + 1:] for param in params]\n",
    "    trainingFeatures[entryNum] = np.array([float(param) for param in cleanedParams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "#trainingFeatures = trainingFeatures[:,[6,7,13]]\n",
    "trainingFeatures = X_scaler.fit_transform(trainingFeatures)\n",
    "trainingFeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exract test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTestData = int(input00[numTrainingData+1])\n",
    "print(numTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testFeatures = np.zeros(shape=(numTestData, numFeatures))\n",
    "testNames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputTestFeatures = input00[numTrainingData+2:numTrainingData+2+numTestData]\n",
    "#print(inputTestFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputTestFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainingLabels = convert_to_onehot(trainingLabels,2)\n",
    "trainingLabels = np.array([i + 1 if i < 0 else i for i in trainingLabels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean test dataset, remove unnecessary characters and store in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entryNum in range(testFeatures.shape[0]):\n",
    "    entry = inputTestFeatures[entryNum].split()\n",
    "    testNames.append(entry[0])\n",
    "    params = [entry[i] for i in range(1, numFeatures + 1)]\n",
    "    cleanedParams = [param[param.index(\":\") + 1:] for param in params]\n",
    "    testFeatures[entryNum] = np.array([float(param) for param in cleanedParams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "#testFeatures = testFeatures[:,[6,7,13]] \n",
    "testFeatures = X_scaler.fit_transform(testFeatures)\n",
    "testFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_onehot(labels, num_classes):\n",
    "    labels = labels.reshape(-1)\n",
    "    return np.eye(num_classes, dtype=int)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(features, labels, idx, batch_size):\n",
    "    idx = idx % (features.shape[0] // batch_size)\n",
    "    if (idx+1) * batch_size < labels.shape[0]:\n",
    "        return features[idx * batch_size:(idx+1) * batch_size], labels[idx * batch_size:(idx+1) * batch_size]\n",
    "    else:\n",
    "        return features[idx * batch_size:], labels[idx * batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read labels for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = open(\"output00.txt\").readlines()\n",
    "content = [line.strip('\\n') for line in content]\n",
    "cleanedContent = [entry[entry.index(\" \") + 1:] for entry in content]\n",
    "\n",
    "testLabels = np.array([float(s) for s in cleanedContent], dtype=int)\n",
    "testLabels = np.array([i + 1 if i < 0 else i for i in testLabels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFeatures[:,1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainingFeatures[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "hidden_nodes_layer1 = 128\n",
    "\n",
    "hidden_nodes_layer2 = 64\n",
    "\n",
    "hidden_nodes_layer3 = 32\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float64, name='dropout_keep_prob')\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, trainingFeatures.shape[1]], name='X')\n",
    "y_ = tf.placeholder(tf.int32, [None], name='y_')\n",
    "\n",
    "W_hidden1 = tf.Variable(tf.random_normal([trainingFeatures.shape[1], hidden_nodes_layer1], dtype=tf.float64), name='W_hidden1')\n",
    "b_hidden1 = tf.Variable(tf.random_normal([hidden_nodes_layer1], dtype=tf.float64), name='b_hidden1')\n",
    "\n",
    "hidden1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden1), b_hidden1))\n",
    "\n",
    "\n",
    "W_hidden2 = tf.Variable(tf.random_normal([hidden_nodes_layer1, hidden_nodes_layer2], dtype=tf.float64), name='W_hidden2')\n",
    "b_hidden2 = tf.Variable(tf.random_normal([hidden_nodes_layer2], dtype=tf.float64), name='b_hidden2')\n",
    "\n",
    "hidden2 = tf.nn.relu(tf.add(tf.matmul(hidden1, W_hidden2), b_hidden2))\n",
    "\n",
    "W_hidden3 = tf.Variable(tf.random_normal([hidden_nodes_layer2, hidden_nodes_layer3], dtype=tf.float64), name='W_hidden3')\n",
    "b_hidden3 = tf.Variable(tf.random_normal([hidden_nodes_layer3], dtype=tf.float64), name='b_hidden3')\n",
    "\n",
    "hidden3 = tf.nn.relu(tf.add(tf.matmul(hidden2, W_hidden3), b_hidden3))\n",
    "\n",
    "dropout = tf.nn.dropout(hidden3, keep_prob=dropout_keep_prob)\n",
    "\n",
    "W_out = tf.Variable(tf.random_normal([hidden_nodes_layer3, num_labels], dtype=tf.float64), name='W_out')\n",
    "b_out = tf.Variable(tf.random_normal([num_labels], dtype=tf.float64), name='b_out')\n",
    "\n",
    "y = tf.add(tf.matmul(dropout, W_out), b_out)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y),1), tf.argmax(y_,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64), name='accuracy')\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "merged = tf.summary.merge_all()\n",
    "train_step = tf.train.AdamOptimizer(learning_rate, epsilon=1e-8, beta1=0.9, beta2=0.999).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = './logs'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    checkpoint_file = tf.train.latest_checkpoint('checkpoint/')\n",
    "    if checkpoint_file is not None:\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "\n",
    "    for i in range(10000000):\n",
    "        _, loss, train_summary = sess.run([train_step, cross_entropy, merged], feed_dict={X: trainingFeatures, y_:trainingLabels, dropout_keep_prob:0.3})\n",
    "        train_writer.add_summary(train_summary, i)\n",
    "        if i%100 == 0:\n",
    "            saver.save(sess, save_path='checkpoint/lin_classifier', global_step=i)\n",
    "            print('step {}:{}'.format(i, loss))\n",
    "        if i%100 == 0:\n",
    "            accuracy_out, test_loss, test_summary = sess.run([accuracy,cross_entropy, merged], feed_dict={X: testFeatures, y_:testLabels, dropout_keep_prob:1.0})\n",
    "            test_writer.add_summary(test_summary, i)\n",
    "            print('\\nTest loss:', test_loss)\n",
    "            print('Accuracy:', accuracy_out)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
